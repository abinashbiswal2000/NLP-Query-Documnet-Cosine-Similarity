{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3674c20f-1531-44ee-b5c8-91f31607aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import bisect\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320c7c32-6992-45d9-87fc-97653f2c3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythonBinarySearch(l, element):\n",
    "    \n",
    "    index = bisect.bisect_left(l, element)\n",
    "    \n",
    "    if index < len(l) and l[index] == element:\n",
    "        return True  # Element found\n",
    "    else:\n",
    "        return False  # Element not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0561d335-4ecf-4963-b048-7912b17f2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verified , Works Perfectly\n",
    "\n",
    "def tokenizeSpeech ( stringOfSpeech ) :\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    tokenList = tokenizer.tokenize(stringOfSpeech)\n",
    "    \n",
    "    return(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e2c955-6bd5-483b-9e50-1d2170640eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsAndReturnNewList (listOfTokens) :\n",
    "    \n",
    "    stopWordsList = stopwords.words('english')\n",
    "    stopWordsList.sort()\n",
    "\n",
    "    stopWordsRemovedListOfDocument = list()\n",
    "\n",
    "    # Using binary search to check for stop words in list of tokens\n",
    "\n",
    "    for token in listOfTokens:\n",
    "                \n",
    "        # if not (index < len(stopWordsList) and stopWordsList[index] == token):\n",
    "        if not (pythonBinarySearch(stopWordsList , token)):\n",
    "            stopWordsRemovedListOfDocument.append(token)\n",
    "\n",
    "        \n",
    "    return stopWordsRemovedListOfDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4acd53d-b663-4195-85c5-819ff655af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPorterStemmerAndReturnNewList (r) :\n",
    "\n",
    "    stemmer = PorterStemmer()    \n",
    "\n",
    "    newList = list()\n",
    "\n",
    "    for i in r:\n",
    "        newList.append(stemmer.stem(i))\n",
    "\n",
    "    newList.sort()\n",
    "\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b209af-4d49-401b-b940-c35db03191fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTermFrequencyAndReturnDictionary (sortedStopWordsList) :\n",
    "    d = dict()\n",
    "    for i in range(len(sortedStopWordsList)):\n",
    "        if i == 0:\n",
    "            d[sortedStopWordsList[i]] = [1]\n",
    "            continue\n",
    "        if sortedStopWordsList[i] == sortedStopWordsList[i-1]:\n",
    "            d[sortedStopWordsList[i]][0] = d[sortedStopWordsList[i]][0] + 1\n",
    "        else:\n",
    "            d[sortedStopWordsList[i]] = [1]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f62da73-1b82-43f1-89b8-8e12ebfbbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyLogTf (d) :\n",
    "    import math\n",
    "    for i in list(d.keys()):\n",
    "        l = d[i]\n",
    "        l.append(1 + math.log10(l[0]))\n",
    "        d[i] = l\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b4c0ec-b952-47e8-ac00-b512f08485a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateDictionaryWithIDFValues (d) :\n",
    "    import math\n",
    "    \n",
    "    allDocumentKeys = []\n",
    "    for i in d:\n",
    "        allDocumentKeys.append(i.keys())\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        keysListOfCurrentDocument = d[i].keys()\n",
    "        for key in keysListOfCurrentDocument:\n",
    "            df = 0\n",
    "            for k in range(len(d)):\n",
    "                if pythonBinarySearch(list(d[k].keys()),key):\n",
    "                    df += 1\n",
    "            l = d[i][key]\n",
    "            l.append(df)\n",
    "            idf = math.log10(40/df)\n",
    "            l.append(idf)\n",
    "            tfidf = l[1] * l[3]\n",
    "            l.append(tfidf)\n",
    "            d[i][key] = l\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8284883-40f1-4a52-9794-4818e3a10f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tfidf_values(d):\n",
    "    \n",
    "    for i in range(len(d)):\n",
    "        norm = 0\n",
    "        for j in d[i].keys():\n",
    "            norm += (d[i][j][4] ** 2)\n",
    "        norm = norm ** (1/2)\n",
    "\n",
    "        for j in d[i].keys():\n",
    "            l = d[i][j]\n",
    "            l.append(l[4]/norm)\n",
    "            d[i][j] = l\n",
    "            # d[i][j][4] = (d[i][j][4]/norm)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3d0ae9-8ca2-4cf5-832c-de1d5db167dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAllFilesAndReturnNormalized_tfidf_dict ():\n",
    "    \n",
    "    d = []\n",
    "    \n",
    "    folder_path = './US_Inaugural_Addresses'\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "    \n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='windows-1252') as file:\n",
    "                \n",
    "                doc_content = file.read()\n",
    "                doc_content = doc_content.lower()\n",
    "                \n",
    "                t = tokenizeSpeech(doc_content)\n",
    "                s = removeStopWordsAndReturnNewList(t)\n",
    "                p = applyPorterStemmerAndReturnNewList(s)\n",
    "                tf = findTermFrequencyAndReturnDictionary(p)\n",
    "                lg = applyLogTf(tf)\n",
    "                \n",
    "                d.append(lg)\n",
    "            \n",
    "    tfidf_dict = populateDictionaryWithIDFValues(d)\n",
    "    normalized_tfidf_dict = normalize_tfidf_values(tfidf_dict)\n",
    "    \n",
    "    return normalized_tfidf_dict\n",
    "#----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269e1fad-1677-42a6-ba48-25e8ca91b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getidf (normalString) :\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    tokenList = tokenizer.tokenize(normalString)\n",
    "\n",
    "    if len(tokenList) == 0:\n",
    "        return -1\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stemmedWord = stemmer.stem(tokenList[0])\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    \n",
    "    tokenListDocumentWise = []\n",
    "    \n",
    "    folder_path = './US_Inaugural_Addresses'\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        \n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='windows-1252') as file:\n",
    "                # Print the name of the file\n",
    "                # print(f\"Reading file: {filename\n",
    "                # print(filename)\n",
    "                \n",
    "                doc_content = file.read()\n",
    "                doc_content = doc_content.lower()\n",
    "                \n",
    "                t = tokenizeSpeech(doc_content)\n",
    "                s = removeStopWordsAndReturnNewList(t)\n",
    "                p = applyPorterStemmerAndReturnNewList(s)\n",
    "                \n",
    "                tokenListDocumentWise.append(p)\n",
    "\n",
    "    df = 0\n",
    "    \n",
    "    for i in tokenListDocumentWise:\n",
    "        if pythonBinarySearch (i, stemmedWord) :\n",
    "            df += 1\n",
    "        \n",
    "    if df == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return math.log10(40/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a55b71-51f1-4028-82ef-811963324f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessToken (token):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    tokenList = tokenizer.tokenize(token)\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    preprocessedToken = stemmer.stem(tokenList[0])\n",
    "\n",
    "    return preprocessedToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beedcfab-32eb-4236-93ae-29f0b64efc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tfidf_dict = readAllFilesAndReturnNormalized_tfidf_dict()    \n",
    "\n",
    "def getweight (filename,token):\n",
    "    \n",
    "    targetIndex = filename.split('_')\n",
    "    if targetIndex[0][0] == \"0\":\n",
    "        targetIndex = int(targetIndex[0][1]) - 1\n",
    "    else:\n",
    "        targetIndex = int(targetIndex[0]) - 1\n",
    "\n",
    "    # pre process the token\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    \n",
    "    tokenList = tokenizer.tokenize(token)\n",
    "\n",
    "    if len(tokenList) == 0:\n",
    "        # No tokenization possible\n",
    "        # For example - 1234@#$()*\n",
    "        return 0\n",
    "\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    preprocessedToken = stemmer.stem(tokenList[0])\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    \n",
    "    l = list(normalized_tfidf_dict[targetIndex].keys())\n",
    "    \n",
    "    if pythonBinarySearch(l , preprocessedToken):\n",
    "        return normalized_tfidf_dict[targetIndex][preprocessedToken][5]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd3f2c7-4739-4b8d-b994-8f18b72980d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeQueryVector (queryDict):\n",
    "    norm = 0\n",
    "    for i in list(queryDict.keys()):\n",
    "        norm += (queryDict[i][1] ** 2)\n",
    "    norm = norm ** (1/2)\n",
    "        \n",
    "    for i in list(queryDict.keys()):\n",
    "        # norm += (queryDict[i][1] ** 2)\n",
    "        l = queryDict[i]\n",
    "        l.append(l[1]/norm)\n",
    "        queryDict[i] = l\n",
    "\n",
    "    return queryDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2319c8ec-ed9d-4a41-98f3-39659905b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNormalizedQueryVector (q_document) :\n",
    "    tokensList = tokenizeSpeech(q_document)\n",
    "    s_w_r = removeStopWordsAndReturnNewList(tokensList)\n",
    "    p = applyPorterStemmerAndReturnNewList(s_w_r)\n",
    "    tf = findTermFrequencyAndReturnDictionary(p)\n",
    "    ltf = applyLogTf(tf)\n",
    "    n = normalizeQueryVector(ltf)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe98b07a-15e1-4359-9b92-0265b70ac06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.1 - Creating Posting List\n",
    "\n",
    "postingDict = {}\n",
    "\n",
    "for documentNumber in range(len(normalized_tfidf_dict)):\n",
    "    for token in normalized_tfidf_dict[documentNumber].keys():\n",
    "        if postingDict.get(token, False) == False:\n",
    "            tf_idf_value = normalized_tfidf_dict[documentNumber][token][5]\n",
    "            postingDict[token] = [[documentNumber , tf_idf_value]]\n",
    "        else:\n",
    "            tf_idf_value = normalized_tfidf_dict[documentNumber][token][5]\n",
    "            l = postingDict[token]\n",
    "            l.append([documentNumber , tf_idf_value])\n",
    "            postingDict[token] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce8f3faf-8fa8-43a2-afeb-0a657c47d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.1 - Sorting Posting List in Decending Order\n",
    "\n",
    "for i in list(postingDict.keys()):\n",
    "    l = postingDict[i]\n",
    "    sorted_data = sorted(l, key=lambda x: x[1], reverse=True)\n",
    "    postingDict[i] = sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5de0deb1-b55f-4083-b46e-0444d46c34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.2 - return the top 10 documents based on the TF-IDF weights for each token in query.\n",
    "\n",
    "\n",
    "def query (queryString) :\n",
    "    # ----------------------------------------------------------\n",
    "    numberNonsense = False\n",
    "    try:\n",
    "        # print(queryString.split(\" \"))\n",
    "        for i in queryString.split(\" \"):\n",
    "            x = preprocessToken(i)\n",
    "    except Exception as e:\n",
    "        numberNonsense = True\n",
    "\n",
    "\n",
    "    if numberNonsense == True:\n",
    "        return f\"( None , 0 )\"\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    l = makeNormalizedQueryVector(queryString)\n",
    "    # print(l)\n",
    "    # for i in l:\n",
    "    #     print(f'{i}: ', l[i])\n",
    "    ll = list(l.keys())\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    uselessTokensInQuery = False\n",
    "    for token in ll:\n",
    "        if postingDict.get(token , False) == False:\n",
    "            # print(token)\n",
    "            uselessTokensInQuery = True\n",
    "            break\n",
    "\n",
    "    if uselessTokensInQuery == True:\n",
    "        return f\"( None , 0 )\"\n",
    "    # -----------------------------------------------------\n",
    "    \n",
    "    d = {}\n",
    "    for token in ll:\n",
    "        if postingDict.get(token , False) == False:\n",
    "            continue\n",
    "        else:\n",
    "            d[token] = postingDict[token][0:10]\n",
    "    \n",
    "    setsList = []\n",
    "    for key in d.keys():\n",
    "        q = []\n",
    "        for i in d[key]:\n",
    "            q.append(i[0])\n",
    "        setsList.append(set(q))\n",
    "    \n",
    "    # setsList\n",
    "    commonDocx = set.intersection(*setsList)\n",
    "    if len(commonDocx) == 0:\n",
    "        return f\"( Fetch More , 0 )\"    \n",
    "\n",
    "    # print(commonDocx)\n",
    "        \n",
    "    # -----------------------------------------------------    \n",
    "    \n",
    "    \n",
    "    storage = list()\n",
    "    for documentNumber in commonDocx:\n",
    "        d_temp = {}\n",
    "        for key in d.keys():\n",
    "            lll = d[key]\n",
    "            for tf_idf_value_of_posting_dict in lll:\n",
    "                if tf_idf_value_of_posting_dict[0] == documentNumber:\n",
    "                    d_temp[key] = tf_idf_value_of_posting_dict[1]\n",
    "                    break\n",
    "        cosine = 0\n",
    "        # print(d_temp)\n",
    "        # print(l)\n",
    "        # print(d_temp)\n",
    "        for key in d_temp.keys():\n",
    "            cosine += (d_temp[key] * l[key][-1])\n",
    "            # print(f\"documentNumber: {documentNumber}, token: {key}, {d_temp[key]:.2f} * {l[key][-1]:.2f}\")\n",
    "    \n",
    "        storage.append([documentNumber+1 , cosine])\n",
    "    \n",
    "    \n",
    "    maxIndex = 0\n",
    "    cosine = 0\n",
    "    for i in storage:\n",
    "        if i[1] > cosine:\n",
    "            maxIndex = i[0]\n",
    "            cosine = i[1]\n",
    "\n",
    "    # -----------------------------------------------------    \n",
    "\n",
    "    \n",
    "    fileNames = {\n",
    "        1: '01_washington_1789.txt',\n",
    "        2: '02_washington_1793.txt',\n",
    "        3: '03_adams_john_1797.txt',\n",
    "        4: '04_jefferson_1801.txt',\n",
    "        5: '05_jefferson_1805.txt',\n",
    "        6: '06_madison_1809.txt',\n",
    "        7: '07_madison_1813.txt',\n",
    "        8: '08_monroe_1817.txt',\n",
    "        9: '09_monroe_1821.txt',\n",
    "        10: '10_adams_john_quincy_1825.txt',\n",
    "        11: '11_jackson_1829.txt',\n",
    "        12: '12_jackson_1833.txt',\n",
    "        13: '13_van_buren_1837.txt',\n",
    "        14: '14_harrison_1841.txt',\n",
    "        15: '15_polk_1845.txt',\n",
    "        16: '16_taylor_1849.txt',\n",
    "        17: '17_pierce_1853.txt',\n",
    "        18: '18_buchanan_1857.txt',\n",
    "        19: '19_lincoln_1861.txt',\n",
    "        20: '20_lincoln_1865.txt',\n",
    "        21: '21_grant_1869.txt',    \n",
    "        22: '22_grant_1873.txt',\n",
    "        23: '23_hayes_1877.txt',\n",
    "        24: '24_garfield_1881.txt',\n",
    "        25: '25_cleveland_1885.txt',\n",
    "        26: '26_harrison_1889.txt',\n",
    "        27: '27_cleveland_1893.txt',\n",
    "        28: '28_mckinley_1897.txt',\n",
    "        29: '29_mckinley_1901.txt',\n",
    "        30: '30_roosevelt_theodore_1905.txt',\n",
    "        31: '31_taft_1909.txt',\n",
    "        32: '32_wilson_1913.txt',\n",
    "        33: '33_wilson_1917.txt',\n",
    "        34: '34_harding_1921.txt',\n",
    "        35: '35_coolidge_1925.txt',\n",
    "        36: '36_hoover_1929.txt',\n",
    "        37: '37_roosevelt_franklin_1933.txt',\n",
    "        38: '38_roosevelt_franklin_1937.txt',\n",
    "        39: '39_roosevelt_franklin_1941.txt',\n",
    "        40: '40_roosevelt_franklin_1945.txt'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return f\"( File Name: {fileNames[maxIndex]} , Cosine: {cosine} )\"\n",
    "    \n",
    "    # -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57523190-f905-4de5-b1b3-aeb51184c342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cb663-d319-4588-937e-6facc56cf4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22409748-1b12-451a-9705-2c83ef487484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "0.698970004336\n",
      "0.187086643357\n",
      "0.057991946978\n",
      "0.139661993429\n",
      "0.033858267261\n",
      "--------------------------------------------\n",
      "0.006537500538\n",
      "0.008278952636\n",
      "0.002917313392\n",
      "0.028125012259\n",
      "0.077484582245\n",
      "--------------------------------------------\n",
      "( File Name: 21_grant_1869.txt , Cosine: 0.016899649589301367 )\n",
      "( File Name: 20_lincoln_1865.txt , Cosine: 0.1267078533053904 )\n",
      "( File Name: 07_madison_1813.txt , Cosine: 0.08756176974000539 )\n",
      "( Fetch More , 0 )\n",
      "( File Name: 22_grant_1873.txt , Cosine: 0.010700041426546273 )\n",
      "\n",
      "\n",
      "Time taken for execution = 18.19 Seconds\n"
     ]
    }
   ],
   "source": [
    "# s = time.time()\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "print(\"%.12f\" % getidf('democracy'))\n",
    "print(\"%.12f\" % getidf('foreign'))\n",
    "print(\"%.12f\" % getidf('states'))\n",
    "print(\"%.12f\" % getidf('honor'))\n",
    "print(\"%.12f\" % getidf('great'))\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "print(\"%.12f\" % getweight('19_lincoln_1861.txt','constitution'))\n",
    "print(\"%.12f\" % getweight('23_hayes_1877.txt','public'))\n",
    "print(\"%.12f\" % getweight('25_cleveland_1885.txt','citizen'))\n",
    "print(\"%.12f\" % getweight('09_monroe_1821.txt','revenue'))\n",
    "print(\"%.12f\" % getweight('37_roosevelt_franklin_1933.txt','leadership'))\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "print(query(\"states laws\"))\n",
    "print(query(\"war offenses\"))\n",
    "print(query(\"british war\"))\n",
    "print(query(\"texas government\"))\n",
    "print(query(\"world civilization\"))\n",
    "\n",
    "e = time.time()\n",
    "print(f'\\n\\nTime taken for execution = {(e-s):.2f} Seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
